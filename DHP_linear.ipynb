{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from system_dynamics import Sys_dynamics\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define actor\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, n_state, n_control, n_nn_a, u_b):\n",
    "        super(Actor, self).__init__()\n",
    "        self.n_state = n_state\n",
    "        self.n_control = n_control\n",
    "        self.n_nn = n_nn_a\n",
    "        self.u_b = u_b\n",
    "        self.linear1 = nn.Linear(self.n_state, self.n_nn,bias=False)\n",
    "        self.linear2 = nn.Linear(self.n_nn, self.n_nn,bias=False)\n",
    "        self.linear3 = nn.Linear(self.n_nn, self.n_control,bias=False)    \n",
    "        # no bias because when x = 0, u = 0\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = torch.tanh(self.linear1(state))\n",
    "        output = torch.tanh(self.linear2(output))\n",
    "        D_nn = self.linear3(output)\n",
    "        output = self.u_b*torch.tanh(D_nn)\n",
    "        # output through the bounding layer\n",
    "        return output, D_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define critic\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, n_state, n_nn_c):\n",
    "        super(Critic, self).__init__()\n",
    "        self.n_state = n_state\n",
    "        self.n_nn = n_nn_c\n",
    "        self.linear1 = nn.Linear(self.n_state, self.n_nn, bias=False)\n",
    "        self.linear2 = nn.Linear(self.n_nn, self.n_nn, bias=False)\n",
    "        self.linear3 = nn.Linear(self.n_nn, n_state, bias=False)\n",
    "        # no bias because when x = 0, u = 0\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = torch.tanh(self.linear1(state))\n",
    "        output = torch.tanh(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        # no bounding layer\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor(\n",
      "  (linear1): Linear(in_features=6, out_features=10, bias=False)\n",
      "  (linear2): Linear(in_features=10, out_features=10, bias=False)\n",
      "  (linear3): Linear(in_features=10, out_features=2, bias=False)\n",
      ")\n",
      "Critic(\n",
      "  (linear1): Linear(in_features=6, out_features=10, bias=False)\n",
      "  (linear2): Linear(in_features=10, out_features=10, bias=False)\n",
      "  (linear3): Linear(in_features=10, out_features=6, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# config networks\n",
    "n_nn_c = 10\n",
    "n_nn_a = 10\n",
    "\n",
    "\n",
    "# system settings\n",
    "n_state = 6\n",
    "n_control = 2\n",
    "u_b = 20*np.pi/180 # input constraints\n",
    "\n",
    "actor = Actor(n_state= n_state, n_control=n_control,n_nn_a=n_nn_a, u_b=u_b)\n",
    "critic = Critic(n_state= n_state, n_nn_c=n_nn_c)\n",
    "\n",
    "print(actor)\n",
    "print(critic)\n",
    "\n",
    "\n",
    "sys_dynamics = Sys_dynamics() # instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define trianing iterations\n",
    "def train_iters(actor, critic, state_now, Q, R, gamma_c, num_epochs, time, dt):\n",
    "# NNs are trained within this function\n",
    "# outside this function, no torch is used\n",
    "\n",
    "    optimizer_actor = torch.optim.SGD(actor.parameters(), lr = 0.0001)\n",
    "    optimizer_critic = torch.optim.SGD(critic.parameters(), lr = 0.0001)\n",
    "    loss_actor_fn = torch.nn.MSELoss()\n",
    "    loss_critic_fn = torch.nn.MSELoss()\n",
    "    # initialization\n",
    "    state_now = torch.tensor(state_now, dtype=torch.float32, requires_grad= True)\n",
    "    Q = torch.diag(torch.tensor(Q, dtype=torch.float32))\n",
    "    R = torch.diag(torch.tensor(R, dtype=torch.float32))\n",
    "    # state_now is used to compute grad\n",
    "\n",
    "    # main loop\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        state_now_model = state_now.detach().requires_grad_()\n",
    "        # actor estimation\n",
    "        control_now_model, _ = actor(state_now_model)\n",
    "        control_now_model.retain_grad()\n",
    "        # model estimation\n",
    "        \n",
    "        A = torch.Tensor([[-0.3220,0.0640,0.0364,-0.9917,0.0003,0.0008],\n",
    "        [0,0,1,0.0037,0,0],\n",
    "        [-30.6492,0,-3.6784,0.6646,-0.7333,0.1315],\n",
    "        [8.5396,0,-0.0254,-0.4764,-0.0319,-0.0620],\n",
    "        [0,0,0,0,-20.2,0],\n",
    "        [0,0,0,0,0,-20.2],\n",
    "        ])\n",
    "        B = torch.Tensor([[0,0],[0,0],[0,0],[0,0],\n",
    "        [20.2,0],[0,20.2]])\n",
    "        d_x = torch.matmul(A,state_now_model) +  torch.matmul(B,control_now_model)\n",
    "        state_next = d_x*dt + state_now_model\n",
    "\n",
    "\n",
    "        # compute model\n",
    "        # d_state_next_state_now = f + g*d_u_x\n",
    "        # d_state_next_control_now = g\n",
    "        d_state_next_state_now = torch.tensor([],dtype=torch.float32)\n",
    "        d_state_next_control_now = torch.tensor([],dtype=torch.float32)\n",
    "\n",
    "        len_state_next = state_next.size()[0]\n",
    "        for k in range(len_state_next):\n",
    "            state_now_model.grad = torch.tensor(np.zeros(n_state),dtype=torch.float32)\n",
    "            control_now_model.grad = torch.tensor(np.zeros(n_control),dtype=torch.float32)\n",
    "            if k != len_state_next-1:\n",
    "                state_next[k].backward(retain_graph=True)\n",
    "            else:\n",
    "                state_next[k].backward()\n",
    "            d_state_next_state_now = torch.cat((d_state_next_state_now,state_now_model.grad.unsqueeze(0)))\n",
    "            d_state_next_control_now = torch.cat((d_state_next_control_now,control_now_model.grad.unsqueeze(0)))\n",
    "\n",
    "        # for critic\n",
    "        state_next_critic = state_next.detach()\n",
    "        state_now_critic = state_now.detach()\n",
    "        # critic estimation\n",
    "        lambda_now = critic(state_now_critic)\n",
    "        lambda_next = critic(state_next_critic)\n",
    "\n",
    "\n",
    "        # compute optimal control policy\n",
    "        D = -0.5*gamma_c/u_b*torch.matmul(torch.matmul(torch.linalg.inv(R),d_state_next_control_now.T), lambda_next)\n",
    "        control_opti = u_b*torch.tanh(D)\n",
    "\n",
    "        # compute control to update actor\n",
    "        # recompute to get clean weights\n",
    "        control_now_actor, D_nn = actor(state_now) # state_now is clean so far\n",
    "\n",
    "        # actor's loss\n",
    "        loss_actor = loss_actor_fn(control_now_actor, control_opti.detach() + torch.Tensor(disturb_sin(time)))\n",
    "\n",
    "\n",
    "        # compute reward\n",
    "        Y1 = torch.dot(torch.tanh(D_nn), D_nn)\n",
    "        Y2 = 0.5*torch.log(1-torch.tanh(D_nn)**2)\n",
    "        Y = 2*torch.dot(torch.diag(R)*(u_b**2),(Y1+Y2))\n",
    "        \n",
    "        reward = torch.matmul(torch.matmul(state_now.T,Q),state_now) + Y\n",
    "\n",
    "        reward.backward(retain_graph=True)\n",
    "       \n",
    "\n",
    "        # critic's loss\n",
    "        loss_critic = loss_critic_fn(lambda_now, (gamma_c*torch.matmul(lambda_next,d_state_next_state_now) +  state_now.grad.T).detach())\n",
    "\n",
    "        # update NNs\n",
    "        optimizer_actor.zero_grad()\n",
    "        optimizer_critic.zero_grad()\n",
    "        loss_actor.backward()\n",
    "        loss_critic.backward()\n",
    "        optimizer_actor.step()\n",
    "        optimizer_critic.step()\n",
    "        # print(\"u:\", control_now_actor,\"E_a:\",loss_actor.data, \"E_c:\",loss_critic.data, \"r:\", reward)\n",
    "\n",
    "        \n",
    "\n",
    "        # print('loss_a:', loss_actor.item(),'loss_c:', loss_critic.item(),'epoch:', epoch)\n",
    "    return control_now_actor.detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disturb_sin(t):\n",
    "    u_e = -0.8*np.pi/180*np.exp(-2*t)*((np.sin(100*t))**2*np.cos(100*t) + (np.sin(2*t))**2*np.cos(0.1*t) +(np.sin(1.2*t))**2*np.cos(0.5*t) + (np.sin(t))**5 )\n",
    "    return np.array([u_e, -u_e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_steps = 2000 # running time\n",
    "num_epochs = 20 # \n",
    "dt = 0.01\n",
    "\n",
    "# initial system\n",
    "state_now = np.array([30*np.pi/180,0,0,0,0,0])\n",
    "control_now = np.array([0,0])\n",
    "# optimal control seetings\n",
    "Q = np.array([5,10,10,5,0,0])\n",
    "R = np.array([1,1])\n",
    "gamma_c = 1\n",
    "# noises\n",
    "x1_std = ((1.8e-3)*np.pi/180);\n",
    "x2_std = ((3e-2)*np.pi/180);\n",
    "x3_std = ((3e-2)*np.pi/180);\n",
    "x4_std = ((1.8e-3)*np.pi/180);\n",
    "x5_std = ((4e-2)*np.pi/180);\n",
    "x6_std = ((4e-2)*np.pi/180);\n",
    "\n",
    "# store information\n",
    "states = []\n",
    "controls = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 320/2000 [00:15<01:20, 20.76it/s]"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    \n",
    "    # train_iters(model, actor, critic, state_now, Q, R, gamma_c, num_epochs)\n",
    "    for step in tqdm(range(t_steps)):\n",
    "        time = step*dt\n",
    "        # measurement noises\n",
    "        noises = np.array([np.random.randn()*x1_std, np.random.randn()*x2_std, np.random.randn()*x3_std,\n",
    "        np.random.randn()*x4_std, np.random.randn()*x5_std, np.random.randn()*x6_std])\n",
    "        state_measure = state_now + noises\n",
    "\n",
    "        # compute control & update weights\n",
    "        control_now = train_iters(actor, critic, state_measure, Q, R, gamma_c, num_epochs,time,dt)\n",
    "\n",
    "        # update system\n",
    "        state_next = sys_dynamics.f16_lateral(state_now,control_now,dt)\n",
    "\n",
    "        states.append(state_now*180/np.pi)\n",
    "        controls.append(control_now*180/np.pi)\n",
    "        state_now = state_next\n",
    "    print(\"Done!\")\n",
    "\n",
    "    steps = np.linspace(0,(t_steps-1)*dt,t_steps)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(steps, controls)\n",
    "    plt.title(\"control commands\")\n",
    "    plt.grid(axis='both')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(steps, states)\n",
    "    plt.title(\"states\")\n",
    "    plt.grid(axis='both')\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # print(states)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4eae41719ead7c1053bbd37dfd512a842fc7947755e7ff08ec3f20bb17b726e2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
